import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.document_loaders import PDFMinerLoader
from langchain_openai import OpenAIEmbeddings
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain, create_history_aware_retriever
from langchain_core.messages import HumanMessage, AIMessage

from langchain_community.document_loaders import WikipediaLoader

from dotenv import load_dotenv
import os

# ##########################
# # FASE 0 - Configuraci√≥n #
# ##########################

# Cargar variables de entorno
load_dotenv()
api_key = os.getenv("API_KEY")  # FOR OpenAIEmbeddings Method
os.environ["OPENAI_API_KEY"] = api_key  # FOR ChatOpenAI Method

pdf_name = 'IA.pdf'

# Configuraci√≥n del modelo LLM (Large Language Model)
llm = ChatOpenAI(
    api_key=api_key,
    model="gpt-3.5-turbo",
    temperature=0.8,
    timeout=20,
    max_retries=1
)

# Configuraci√≥n de embeddings y almacenamiento en cache local
embeddings = OpenAIEmbeddings()
store = LocalFileStore("./cache/")

cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    embeddings, store, namespace=embeddings.model
)

wikipedia = WikipediaLoader
# #####################################
# # FASE 1 - PREPARACION DEL CONTEXTO #
# #####################################

# SEGMENTACION DE DOCUMENTOS
# Cargar el documento PDF y dividirlo en partes manejables
loader = PDFMinerLoader(f'../pdfs/{pdf_name}')
data = loader.load()

# Dividir el texto del documento en segmentos m√°s peque√±os para su procesamiento
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=700,
    chunk_overlap=100,
    length_function=len,
    is_separator_regex=False,
)

documents = text_splitter.split_documents(data)

# OBTENER EMBEDDINGS DE LOS DOCUMENTOS
# Obtener los embeddings para los documentos segmentados y almacenarlos en un √≠ndice vectorial
vector = FAISS.from_documents(documents, cached_embedder)
retriever = vector.as_retriever()

# CREAR CADENA DE RECUPERACION Y RESPUESTA
# Crear un recuperador a partir del √≠ndice vectorial y una cadena de recuperaci√≥n usando la cadena de documentos
# Crear una cadena de recuperaci√≥n que tenga en cuenta el historial de la conversaci√≥n...
history_prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    ("user", "Dada la conversaci√≥n anterior, genere una consulta de b√∫squeda para buscar informaci√≥n relevante para la conversaci√≥n")
])

retriever_chain = create_history_aware_retriever(llm, retriever, history_prompt)

# Crear la cadena de documentos considerando el historial y dandole un toque felino
document_prompt = ChatPromptTemplate.from_messages([
    ("system", '''
    Sos un gato asistente.
    Eres un gato amigable, cordial y gracioso, y respondes en espa√±ol bas√°ndote en el contexto provisto desde el documento.
    Aseg√∫rate de que tus respuestas sean √∫tiles y tengan un toque de humor y calidez felina.
    Responda las preguntas del usuario seg√∫n el siguiente contexto
    \n\n
    {context}
    '''),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
])
document_chain_with_history = create_stuff_documents_chain(llm, document_prompt)

# Crear cadena final de recuperaci√≥n combinando la cadena de recuperaci√≥n con historial y la cadena de documentos con historial
retrieval_chain = create_retrieval_chain(retriever_chain, document_chain_with_history)


# ######################################################
# # FASE 2 - PREPARACION DEL HISTORIAL DE CONVERSACION #
# ######################################################
# Funci√≥n para manejar la conversaci√≥n con el historial incluido
def chat_with_history(retrieval_chain, user_input, chat_history):
    chat_history.append(HumanMessage(content=user_input))
    response = retrieval_chain.invoke({
        "chat_history": chat_history,
        "input": user_input
    })
    answer = response["answer"]
    chat_history.append(AIMessage(content=answer))
    return answer, chat_history


# ################
# # FASE 3 - UI  #
# ################
with st.sidebar:
    "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/Luisarg03/StreamlitLangchainBot)"

st.image("./img/logo_cat.png", width=200)
st.title("üí¨ SAMI")
st.caption("üöÄ A Streamlit chatbot powered by OpenAI")

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    if "wikipedia" in prompt.lower():
        response, st.session_state["chat_history"] = chat_with_history(retrieval_chain, prompt, st.session_state["chat_history"])
        prompt = prompt.lower().replace("wikipedia", "")
        st.chat_message("assistant").write("Un momento, busco informacion en Wikipedia...üê±‚ú®")

        messages = [
            (
                "system",
                "Extrae del texto el concepto, idea o palabra a buscar, para generar una palabra o frase clave para buscar en motores de busqueda"
            ),
            ("human", f"{prompt}")
        ]

        concept = llm.invoke(messages).content

        print(concept)

        wikipedia_result = wikipedia(query=concept, load_max_docs=2, lang="es", doc_content_chars_max=3000).load()

        wiki_messages = [
            (
                "system",
                '''
                Eres un exeperto resumiendo textos.
                Desglosas los conceptos m√°s importantes y los presentas de manera clara y concisa.
                Si es necesario, en tu criterio, agregas ejemplos simples y claros para ilustrar los conceptos complejos.
                Si el resultado no coincide con la consulta, simplemente ignora la consulta.
                '''
            ),
            ("human", f"{wikipedia_result}")
        ]

        wiki_response = llm.invoke(wiki_messages).content

        for i in wikipedia_result:
            link = i.metadata['source']
            wiki_response = wiki_response + f"\n\nFuente: {link}"

        st.session_state.messages.append({"role": "assistant", "content": wiki_response})

        st.chat_message("assistant").write(wiki_response)

    else:
        response, st.session_state["chat_history"] = chat_with_history(retrieval_chain, prompt, st.session_state["chat_history"])
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.chat_message("assistant").write(response)
